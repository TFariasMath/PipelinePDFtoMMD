import json
from pathlib import Path

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {"id": "intro"},
            "source": [
                "# Nougat OCR Pipeline\n",
                "\n",
                "Procesamiento de documentos tecnicos complejos."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "setup"},
            "outputs": [],
            "source": [
                "# @title 1. Setup\n",
                "!pip install nougat-ocr pypdf torch tqdm transformers==4.38.2 albumentations==1.4.3 pypdfium2 fpdf2\n",
                "!python -m nltk.downloader words\n",
                "\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import shutil\n",
                "import datetime\n",
                "import hashlib\n",
                "import re\n",
                "import site\n",
                "from pathlib import Path\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB and not os.path.exists('/content/drive'):\n",
                "    drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "gpu_check"},
            "outputs": [],
            "source": [
                "# @title 2. GPU Check\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    device_name = torch.cuda.get_device_name(0)\n",
                "    print(f\"GPU Detectada: {device_name} (OK)\")\n",
                "    print(\"El sistema usará aceleración por hardware para el proceso de OCR.\")\n",
                "else:\n",
                "    print(\"ADVERTENCIA: No se detectó ninguna GPU.\")\n",
                "    print(\"Vaya a 'Entorno de ejecución' > 'Cambiar tipo de entorno de ejecución' y seleccione T4 GPU.\")\n",
                "    print(\"El procesamiento en CPU será significativamente más lento.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "repair"},
            "outputs": [],
            "source": [
                "# @title 3. Fixes\n",
                "def apply_patches():\n",
                "    sp_list = site.getsitepackages()\n",
                "    if not sp_list: return\n",
                "    sp = Path(sp_list[0])\n",
                "    \n",
                "    # Transformers\n",
                "    for p in sp.rglob(\"transformers/configuration_utils.py\"):\n",
                "        content = p.read_text()\n",
                "        if \"PreTrainedConfig = PretrainedConfig\" not in content:\n",
                "            p.write_text(content + \"\\nPreTrainedConfig = PretrainedConfig\\n\")\n",
                "            print(\"Patch Transformers: OK\")\n",
                "            \n",
                "    # Nougat transforms\n",
                "    for p in sp.rglob(\"nougat/transforms.py\"):\n",
                "        content = p.read_text()\n",
                "        if 'from pydantic import BaseModel' in content and 'Field' not in content:\n",
                "            content = content.replace('from pydantic import BaseModel', 'from pydantic import BaseModel, Field')\n",
                "            content = content.replace('compression_type: str = \"webp\"', 'compression_type: str = Field(\"webp\")')\n",
                "            p.write_text(content)\n",
                "            print(\"Patch Nougat Transforms: OK\")\n",
                "\n",
                "    # Rasterizer logic\n",
                "    for p in sp.rglob(\"nougat/dataset/rasterize.py\"):\n",
                "        content = p.read_text()\n",
                "        content = content.replace(\"pypdfium2.PdfDocument(pdf)\", \"pypdfium2.PdfDocument(str(pdf))\")\n",
                "        new_logic = \"\"\"if hasattr(pdf, \"render\"):\n",
                "            renderer = pdf.render(pypdfium2.PdfBitmap.to_pil, page_indices=pages, scale=dpi/72)\n",
                "        elif hasattr(pdf, \"render_topil\"):\n",
                "            renderer = pdf.render_topil(page_indices=pages, scale=dpi/72)\n",
                "        else:\n",
                "            renderer = [pdf[i].render(scale=dpi/72).to_pil() for i in pages]\"\"\"\n",
                "        pattern = r\"renderer\\s*=\\s*pdf\\.render\\(.*?\\)\"\n",
                "        if \"hasattr(pdf, \\\"render\\\")\" not in content:\n",
                "            import re\n",
                "            content = re.sub(pattern, new_logic, content, flags=re.DOTALL)\n",
                "            p.write_text(content)\n",
                "            print(\"Patch Universal Rasterizer: OK\")\n",
                "\n",
                "apply_patches()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "config"},
            "outputs": [],
            "source": [
                "# @title 4. Config\n",
                "BASE_DIR = \"/content/drive/MyDrive/NovaLibrary\" # @param {type:\"string\"}\n",
                "MODEL_SIZE = \"0.1.0-small\" # @param [\"0.1.0-small\", \"0.1.0-base\"]\n",
                "FORCE_REPROCESS = False # @param {type:\"boolean\"}\n",
                "\n",
                "STRUCTURE = {\n",
                "    \"input\": Path(BASE_DIR) / \"input\",\n",
                "    \"output\": Path(BASE_DIR) / \"output\",\n",
                "    \"failed\": Path(BASE_DIR) / \"failed\",\n",
                "    \"checkpoint\": Path(BASE_DIR) / \"checkpoint\"\n",
                "}\n",
                "\n",
                "for p in STRUCTURE.values(): p.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "REGISTRY_PATH = STRUCTURE[\"checkpoint\"] / \"registry.json\"\n",
                "LOG_PATH = STRUCTURE[\"checkpoint\"] / \"pipeline.log\"\n",
                "\n",
                "def log_message(msg):\n",
                "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
                "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f: f.write(f\"[{ts}] {msg}\\n\")\n",
                "    print(f\"[{ts}] {msg}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "state"},
            "outputs": [],
            "source": [
                "class PipelineState:\n",
                "    def __init__(self, path):\n",
                "        self.path = path\n",
                "        self.state = self._load()\n",
                "    def _load(self):\n",
                "        if self.path.exists():\n",
                "            with open(self.path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
                "        return {\"processed\": {}, \"failed\": {}}\n",
                "    def save(self):\n",
                "        with open(self.path, \"w\", encoding=\"utf-8\") as f: json.dump(self.state, f, indent=2, ensure_ascii=False)\n",
                "    def is_processed(self, h): return h in self.state[\"processed\"]\n",
                "    def mark_success(self, h, name, out):\n",
                "        self.state[\"processed\"][h] = {\"filename\": name, \"output\": str(out), \"ts\": str(datetime.datetime.now())}\n",
                "        self.save()\n",
                "    def mark_failed(self, h, name, err):\n",
                "        self.state[\"failed\"][h] = {\"filename\": name, \"error\": str(err), \"ts\": str(datetime.datetime.now())}\n",
                "        self.save()\n",
                "\n",
                "def get_file_hash(path):\n",
                "    sha = hashlib.sha256()\n",
                "    with open(path, \"rb\") as f:\n",
                "        while chunk := f.read(8192): sha.update(chunk)\n",
                "    return sha.hexdigest()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "rag"},
            "outputs": [],
            "source": [
                "def extract_structured_data(mmd_path):\n",
                "    print(f\"Buscando estructuras en {mmd_path.name}...\")\n",
                "    with open(mmd_path, \"r\", encoding=\"utf-8\") as f: content = f.read()\n",
                "    equations = re.findall(r\"\\\\\\(.*?\\\\\\)|\\\\\\[.*?\\\\\\]\", content, re.DOTALL)\n",
                "    print(f\"Ecuaciones detectadas: {len(equations)}\")\n",
                "    sections = []; cur_title = \"Preliminares\"; cur_lines = []\n",
                "    for line in content.split(\"\\n\"):\n",
                "        if line.startswith(\"# \") or line.startswith(\"## \"):\n",
                "            if cur_lines: sections.append({\"title\": cur_title, \"content\": \"\\n\".join(cur_lines)})\n",
                "            cur_title = line.replace(\"#\", \"\").strip(); cur_lines = []\n",
                "        else: cur_lines.append(line)\n",
                "    if cur_lines: sections.append({\"title\": cur_title, \"content\": \"\\n\".join(cur_lines)})\n",
                "    print(f\"Secciones identificadas: {len(sections)}\")\n",
                "    return {\n",
                "        \"metadata\": {\"source\": mmd_path.name, \"ts\": str(datetime.datetime.now()), \"eqs\": len(equations), \"secs\": len(sections)},\n",
                "        \"equations\": equations, \"sections\": sections\n",
                "    }\n",
                "def save_structured_json(mmd_path):\n",
                "    try:\n",
                "        data = extract_structured_data(mmd_path)\n",
                "        with open(mmd_path.with_suffix(\".json\"), \"w\", encoding=\"utf-8\") as f: json.dump(data, f, indent=2, ensure_ascii=False)\n",
                "    except Exception as e: log_message(f\"Error JSON: {e}\")\n",
                "\n",
                "def mmd_to_latex(mmd_content, title=\"Documento\"):\n",
                "    body = re.sub(r'\\[MISSING_PAGE_EMPTY:\\d+\\]', '', mmd_content)\n",
                "    protected_math = []\n",
                "    def save_math(m): protected_math.append(m.group(0)); return f\"MATHPROTECT{len(protected_math)-1}Z\"\n",
                "    body = re.sub(r'\\\\\\(.*?\\\\\\)|\\\\\\[.*?\\\\\\\]', save_math, body, flags=re.DOTALL)\n",
                "    raw_cmds_map = {r'\\\\section\\*?\\{([^{}]*)\\}': r'LSECS\\1LEND', r'\\\\subsection\\*?\\{([^{}]*)\\}': r'LSUBS\\1LEND', r'\\\\subsubsection\\*?\\{([^{}]*)\\}': r'LSUBSUBS\\1LEND', r'\\\\paragraph\\*?\\{([^{}]*)\\}': r'LPARAGS\\1LEND', r'\\\\subparagraph\\*?\\{([^{}]*)\\}': r'LSTARTPAGS\\1LEND', r'\\\\textbf\\{([^{}]*)\\}': r'LBOLDS\\1LEND', r'\\\\textit\\{([^{}]*)\\}': r'LITALS\\1LEND', r'\\\\underline\\{([^{}]*)\\}': r'LBOLDS\\1LEND'}\n",
                "    for _ in range(5):\n",
                "        any_ch = False\n",
                "        for rec, sub in raw_cmds_map.items():\n",
                "            body, count = re.subn(rec, sub, body, flags=re.DOTALL)\n",
                "            if count > 0: any_ch = True\n",
                "        if not any_ch: break\n",
                "    body = re.sub(r'^###### (.*)', r'LSTARTPAGS\\1LEND', body, flags=re.MULTILINE)\n",
                "    body = re.sub(r'^##### (.*)', r'LSTARTPAGS\\1LEND', body, flags=re.MULTILINE)\n",
                "    body = re.sub(r'^#### (.*)', r'LPARAGS\\1LEND', body, flags=re.MULTILINE)\n",
                "    body = re.sub(r'^### (.*)', r'LSUBSUBS\\1LEND', body, flags=re.MULTILINE)\n",
                "    body = re.sub(r'^## (.*)', r'LSUBS\\1LEND', body, flags=re.MULTILINE)\n",
                "    body = re.sub(r'^# (.*)', r'LSECS\\1LEND', body, flags=re.MULTILINE)\n",
                "    body = re.sub(r'\\*\\*(.*?)\\*\\*', r'LBOLDS\\1LEND', body)\n",
                "    body = re.sub(r'\\*(.*?)\\*', r'LITALS\\1LEND', body)\n",
                "    special_chars = {'&': r'\\\\&', '%': r'\\\\%', '$': r'\\\\$', '_': r'\\\\_', '{': r'\\\\{', '}': r'\\\\}', '#': r'\\\\#', '~': r'\\\\textasciitilde{}', '^': r'\\\\textasciicircum{}'}\n",
                "    for char, replacement in special_chars.items(): body = body.replace(char, replacement)\n",
                "    body = body.replace(\"LSECS\", r\"\\\\section{\").replace(\"LSUBS\", r\"\\\\subsection{\").replace(\"LSUBSUBS\", r\"\\\\subsubsection{\").replace(\"LPARAGS\", r\"\\\\paragraph{\").replace(\"LSTARTPAGS\", r\"\\\\subparagraph{\").replace(\"LBOLDS\", r\"\\\\textbf{\").replace(\"LITALS\", r\"\\\\textit{\").replace(\"LEND\", \"}\")\n",
                "    def restore_math(m): idx = int(m.group(1)); return protected_math[idx] if idx < len(protected_math) else m.group(0)\n",
                "    body = re.sub(r'MATHPROTECT(\\d+)Z', restore_math, body)\n",
                "    pre = \"\\\\documentclass[11pt,a4paper]{article}\\n\\\\usepackage[utf8]{inputenc}\\n\\\\usepackage[T1]{fontenc}\\n\\\\usepackage[spanish]{babel}\\n\\\\usepackage{amsmath,amssymb,amsfonts}\\n\\\\usepackage{graphicx}\\n\\\\usepackage{geometry}\\n\\\\geometry{margin=1in}\\n\\\\title{ \"+title+\" }\\n\\\\author{Pipeline Nougat OCR}\\n\\\\date{\\\\today}\\n\\\\begin{document}\\n\\\\maketitle\\n\\\\tableofcontents\\n\\\\newpage\\n\"\n",
                "    return pre + body + \"\\n\\\\end{document}\"\n",
                "\n",
                "def generate_blank_page_report(pdf_path, mmd_content, out_p):\n",
                "    try:\n",
                "        import pypdfium2 as pdfium\n",
                "        from fpdf import FPDF\n",
                "        missing = re.findall(r'\\[MISSING_PAGE_EMPTY:(\\d+)\\]', mmd_content)\n",
                "        if not missing: return False\n",
                "        pdf = FPDF(); src = pdfium.PdfDocument(str(pdf_path))\n",
                "        for p in missing:\n",
                "            pdf.add_page(); pdf.set_font(\"Arial\", size=12); pdf.cell(200, 10, txt=f\"Pagina: {p}\", ln=1)\n",
                "            img = src[int(p)-1].render(scale=2).to_pil(); img.save(\"tmp.png\")\n",
                "            pdf.image(\"tmp.png\", x=10, y=30, w=190)\n",
                "        pdf.output(str(out_p)); return True\n",
                "    except Exception as e: log_message(f\"Error Auditor: {e}\"); return False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "main"},
            "outputs": [],
            "source": [
                "# @title 5. Execute\n",
                "def main():\n",
                "    state = PipelineState(REGISTRY_PATH)\n",
                "    all_pdfs = list(STRUCTURE[\"input\"].glob(\"*.pdf\"))\n",
                "    log_message(f\"Encontrados {len(all_pdfs)} archivos.\")\n",
                "    \n",
                "    for pdf_p in all_pdfs:\n",
                "        h = get_file_hash(pdf_p)\n",
                "        if state.is_processed(h) and not FORCE_REPROCESS:\n",
                "            log_message(f\"Saltando {pdf_p.name} (ya procesado).\")\n",
                "            continue\n",
                "        \n",
                "        log_message(f\"--- Procesando: {pdf_p.name} ---\")\n",
                "        try:\n",
                "            !nougat \"{str(pdf_p)}\" -o \"{str(STRUCTURE['output'])}\" --model {MODEL_SIZE} --no-skipping\n",
                "            \n",
                "            out_mmd = STRUCTURE[\"output\"] / f\"{pdf_p.stem}.mmd\"\n",
                "            if out_mmd.exists():\n",
                "                save_structured_json(out_mmd)\n",
                "                # New: LaTeX & Audit\n",
                "                with open(out_mmd, \"r\") as f: mmd = f.read()\n",
                "                with open(out_mmd.with_suffix(\".tex\"), \"w\") as f: f.write(mmd_to_latex(mmd, pdf_p.stem))\n",
                "                audit_p = STRUCTURE[\"output\"] / f\"{pdf_p.stem}_auditoria.pdf\"\n",
                "                generate_blank_page_report(pdf_p, mmd, audit_p)\n",
                "                \n",
                "                state.mark_success(h, pdf_p.name, out_mmd)\n",
                "                log_message(\"Éxito.\")\n",
                "            else:\n",
                "                raise Exception(\"Error: El motor Nougat no generó el archivo de salida.\")\n",
                "        except Exception as e:\n",
                "            log_message(f\"Fallo: {e}\")\n",
                "            state.mark_failed(h, pdf_p.name, str(e))\n",
                "            shutil.move(str(pdf_p), str(STRUCTURE[\"failed\"] / pdf_p.name))\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
        "language_info": {"name": "python", "version": "3.10.12"}
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

with open("nougat_pipeline.ipynb", "w", encoding="utf-8") as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print("Notebook final generado satisfactoriamente.")
