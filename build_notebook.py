import json
from pathlib import Path

notebook = {
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {"id": "intro"},
            "source": [
                "# Pipeline Pro de PDF a Markdown con Nougat\n",
                "**Arquitecto de Sistemas:** IA Personalizada\n",
                "\n",
                "Este notebook es la versión ultra-estable capaz de procesar libros científicos complejos tanto en Colab como en entornos locales.\n",
                "### Mejoras Incluidas:\n",
                "- **Nuclear Repair**: Parchea automáticamente conflictos de versiones en `Transformers`, `Albumentations` y `pypdfium2`.\n",
                "- **Universal Rasterizer**: Soporta múltiples versiones de renderizado de PDF.\n",
                "- **RAG Optimized**: Extracción de ecuaciones y secciones mejorada para IA.\n",
                "- **Force Reprocess**: Permite volver a procesar archivos ya existentes (útil para aplicar parches de recuperación)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "setup"},
            "outputs": [],
            "source": [
                "# @title 1. Instalación de Dependencias Críticas\n",
                "!pip install nougat-ocr pypdf torch tqdm transformers==4.38.2 albumentations==1.4.3 pypdfium2\n",
                "!python -m nltk.downloader words\n",
                "\n",
                "import os\n",
                "import json\n",
                "import time\n",
                "import shutil\n",
                "import datetime\n",
                "import hashlib\n",
                "import re\n",
                "import site\n",
                "from pathlib import Path\n",
                "try:\n",
                "    from google.colab import drive\n",
                "    IN_COLAB = True\n",
                "except:\n",
                "    IN_COLAB = False\n",
                "\n",
                "if IN_COLAB and not os.path.exists('/content/drive'):\n",
                "    drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "gpu_check"},
            "outputs": [],
            "source": [
                "# @title 2. Verificación de Aceleración por Hardware (GPU)\n",
                "import torch\n",
                "if torch.cuda.is_available():\n",
                "    device_name = torch.cuda.get_device_name(0)\n",
                "    print(f\"GPU Detectada: {device_name} (OK)\")\n",
                "    print(\"El sistema usará aceleración por hardware para el proceso de OCR.\")\n",
                "else:\n",
                "    print(\"ADVERTENCIA: No se detectó ninguna GPU.\")\n",
                "    print(\"Vaya a 'Entorno de ejecución' > 'Cambiar tipo de entorno de ejecución' y seleccione T4 GPU.\")\n",
                "    print(\"El procesamiento en CPU será significativamente más lento.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "repair"},
            "outputs": [],
            "source": [
                "# @title 2. Reparación Nuclear (Parches Quirúrgicos)\n",
                "def apply_patches():\n",
                "    sp_list = site.getsitepackages()\n",
                "    if not sp_list: return\n",
                "    sp = Path(sp_list[0])\n",
                "    \n",
                "    # 1. Transformers Bridge\n",
                "    for p in sp.rglob(\"transformers/configuration_utils.py\"):\n",
                "        content = p.read_text()\n",
                "        if \"PreTrainedConfig = PretrainedConfig\" not in content:\n",
                "            p.write_text(content + \"\\nPreTrainedConfig = PretrainedConfig\\n\")\n",
                "            print(\"Patch Transformers: OK\")\n",
                "            \n",
                "    # 2. Nougat Transforms\n",
                "    for p in sp.rglob(\"nougat/transforms.py\"):\n",
                "        content = p.read_text()\n",
                "        if 'from pydantic import BaseModel' in content and 'Field' not in content:\n",
                "            content = content.replace('from pydantic import BaseModel', 'from pydantic import BaseModel, Field')\n",
                "            content = content.replace('compression_type: str = \"webp\"', 'compression_type: str = Field(\"webp\")')\n",
                "            p.write_text(content)\n",
                "            print(\"Patch Nougat Transforms: OK\")\n",
                "\n",
                "    # 3. Universal Rasterizer\n",
                "    for p in sp.rglob(\"nougat/dataset/rasterize.py\"):\n",
                "        content = p.read_text()\n",
                "        content = content.replace(\"pypdfium2.PdfDocument(pdf)\", \"pypdfium2.PdfDocument(str(pdf))\")\n",
                "        new_logic = \"\"\"if hasattr(pdf, \"render\"):\n",
                "            renderer = pdf.render(pypdfium2.PdfBitmap.to_pil, page_indices=pages, scale=dpi/72)\n",
                "        elif hasattr(pdf, \"render_topil\"):\n",
                "            renderer = pdf.render_topil(page_indices=pages, scale=dpi/72)\n",
                "        else:\n",
                "            renderer = [pdf[i].render(scale=dpi/72).to_pil() for i in pages]\"\"\"\n",
                "        pattern = r\"renderer\\s*=\\s*pdf\\.render\\(.*?\\)\"\n",
                "        if \"hasattr(pdf, \\\"render\\\")\" not in content:\n",
                "            import re\n",
                "            content = re.sub(pattern, new_logic, content, flags=re.DOTALL)\n",
                "            p.write_text(content)\n",
                "            print(\"Patch Universal Rasterizer: OK\")\n",
                "\n",
                "apply_patches()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "config"},
            "outputs": [],
            "source": [
                "# @title 3. Configuración de Rutas y Parámetros\n",
                "BASE_DIR = \"/content/drive/MyDrive/NovaLibrary\" # @param {type:\"string\"}\n",
                "MODEL_SIZE = \"0.1.0-small\" # @param [\"0.1.0-small\", \"0.1.0-base\"]\n",
                "FORCE_REPROCESS = False # @param {type:\"boolean\"}\n",
                "\n",
                "STRUCTURE = {\n",
                "    \"input\": Path(BASE_DIR) / \"input\",\n",
                "    \"output\": Path(BASE_DIR) / \"output\",\n",
                "    \"failed\": Path(BASE_DIR) / \"failed\",\n",
                "    \"checkpoint\": Path(BASE_DIR) / \"checkpoint\"\n",
                "}\n",
                "\n",
                "for p in STRUCTURE.values(): p.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "REGISTRY_PATH = STRUCTURE[\"checkpoint\"] / \"registry.json\"\n",
                "LOG_PATH = STRUCTURE[\"checkpoint\"] / \"pipeline.log\"\n",
                "\n",
                "def log_message(msg):\n",
                "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
                "    with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f: f.write(f\"[{ts}] {msg}\\n\")\n",
                "    print(f\"[{ts}] {msg}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "state"},
            "outputs": [],
            "source": [
                "# @title 4. Rastreador de Estado\n",
                "class PipelineState:\n",
                "    def __init__(self, path):\n",
                "        self.path = path\n",
                "        self.state = self._load()\n",
                "    def _load(self):\n",
                "        if self.path.exists():\n",
                "            with open(self.path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
                "        return {\"processed\": {}, \"failed\": {}}\n",
                "    def save(self):\n",
                "        with open(self.path, \"w\", encoding=\"utf-8\") as f: json.dump(self.state, f, indent=2, ensure_ascii=False)\n",
                "    def is_processed(self, h): return h in self.state[\"processed\"]\n",
                "    def mark_success(self, h, name, out):\n",
                "        self.state[\"processed\"][h] = {\"filename\": name, \"output\": str(out), \"ts\": str(datetime.datetime.now())}\n",
                "        self.save()\n",
                "    def mark_failed(self, h, name, err):\n",
                "        self.state[\"failed\"][h] = {\"filename\": name, \"error\": str(err), \"ts\": str(datetime.datetime.now())}\n",
                "        self.save()\n",
                "\n",
                "def get_file_hash(path):\n",
                "    sha = hashlib.sha256()\n",
                "    with open(path, \"rb\") as f:\n",
                "        while chunk := f.read(8192): sha.update(chunk)\n",
                "    return sha.hexdigest()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "rag"},
            "outputs": [],
            "source": [
                "# @title 5. Post-Procesamiento RAG\n",
                "def extract_structured_data(mmd_path):\n",
                "    with open(mmd_path, \"r\", encoding=\"utf-8\") as f: content = f.read()\n",
                "    equations = re.findall(r\"\\\\\\(.*?\\\\\\)|\\\\\\[.*?\\\\\\]\", content, re.DOTALL)\n",
                "    sections = []\n",
                "    curr = {\"title\": \"Preliminares\", \"content\": \"\"}\n",
                "    for line in content.split(\"\\n\"):\n",
                "        if line.startswith(\"# \") or line.startswith(\"## \"):\n",
                "            if curr[\"content\"].strip(): sections.append(curr)\n",
                "            curr = {\"title\": line.replace(\"#\", \"\").strip(), \"content\": \"\"}\n",
                "        else: curr[\"content\"] += line + \"\\n\"\n",
                "    if curr[\"content\"].strip(): sections.append(curr)\n",
                "    return {\n",
                "        \"metadata\": {\"source\": mmd_path.name, \"ts\": str(datetime.datetime.now()), \"eqs\": len(equations), \"secs\": len(sections)},\n",
                "        \"equations\": equations, \"sections\": sections\n",
                "    }\n",
                "def save_structured_json(mmd_path):\n",
                "    try:\n",
                "        data = extract_structured_data(mmd_path)\n",
                "        with open(mmd_path.with_suffix(\".json\"), \"w\", encoding=\"utf-8\") as f: json.dump(data, f, indent=2, ensure_ascii=False)\n",
                "    except Exception as e: log_message(f\"Error JSON: {e}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": None,
            "metadata": {"id": "main"},
            "outputs": [],
            "source": [
                "# @title 6. Ejecución del Pipeline\n",
                "def main():\n",
                "    state = PipelineState(REGISTRY_PATH)\n",
                "    all_pdfs = list(STRUCTURE[\"input\"].glob(\"*.pdf\"))\n",
                "    log_message(f\"Encontrados {len(all_pdfs)} archivos.\")\n",
                "    \n",
                "    for pdf_p in all_pdfs:\n",
                "        h = get_file_hash(pdf_p)\n",
                "        if state.is_processed(h) and not FORCE_REPROCESS:\n",
                "            log_message(f\"Skipping {pdf_p.name} (ya procesado). Activa FORCE_REPROCESS para reintentar.\")\n",
                "            continue\n",
                "        \n",
                "        log_message(f\"--- Procesando: {pdf_p.name} ---\")\n",
                "        try:\n",
                "            !nougat \"{str(pdf_p)}\" -o \"{str(STRUCTURE['output'])}\" --model {MODEL_SIZE} --no-skipping\n",
                "            \n",
                "            out_mmd = STRUCTURE[\"output\"] / f\"{pdf_p.stem}.mmd\"\n",
                "            if out_mmd.exists():\n",
                "                save_structured_json(out_mmd)\n",
                "                state.mark_success(h, pdf_p.name, out_mmd)\n",
                "                log_message(\"Éxito.\")\n",
                "            else:\n",
                "                raise Exception(\"Error: El motor Nougat no generó el archivo de salida.\")\n",
                "        except Exception as e:\n",
                "            log_message(f\"Fallo: {e}\")\n",
                "            state.mark_failed(h, pdf_p.name, str(e))\n",
                "            shutil.move(str(pdf_p), str(STRUCTURE[\"failed\"] / pdf_p.name))\n",
                "\n",
                "if __name__ == \"__main__\":\n",
                "    main()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
        "language_info": {"name": "python", "version": "3.10.12"}
    },
    "nbformat": 4,
    "nbformat_minor": 0
}

with open("nougat_pipeline.ipynb", "w", encoding="utf-8") as f:
    json.dump(notebook, f, indent=1, ensure_ascii=False)

print("Notebook final generado satisfactoriamente.")
